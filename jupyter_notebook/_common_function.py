"""
this file contains all the commonly used functions
"""
import pandas as pd
import numpy as np
import os
from multiprocessing.pool import Pool
import matplotlib.pyplot as plt


def read(file_path):
    '''
    read .csv file and get states
    '''
    if file_path[-4:] == ".csv":
        df = pd.read_csv(file_path)
    elif file_path[-4:] == "xlsx":
        df = pd.read_excel(file_path)
    elif file_path[-4:] == "json":
        df = pd.read_json(file_path)
    else:
        print("can not get file type")
        return None
    print("length of file", len(df))
    print(df.columns)
    print(df.describe())
    df_nona = df.dropna()
    print("length of file w/o NaN", len(df_nona))
    df_nod = df.drop_duplicates()
    print("length of file w/o duplicates", len(df_nod))
    print("------data_sample------")
    print(df.iloc[0][df.columns.tolist()])
    return df


def analyze_1_column(df, column, plot="no"):
    '''
    analyze a particular column in a dataframe
    '''
    print(df[column].describe(), "\n")
    num = len(df[column].describe())
    num_nona = len(df) - len(df[[column]].dropna())
    print("number of missing values is %d" % (num_nona))
    if num <= 4:
        print("%s is not a numerical value" % (column))
        if plot == "yes":
            pplot = df[column].value_counts().plot(kind='bar', figsize=(10, 5))
            pplot.set_title("Number of Samples by %s" % column)
            pplot.set_xlabel(column)
            pplot.set_ylabel("Number of Samples")
            for tick in pplot.get_xticklabels():
                tick.set_rotation(45)
    else:
        print("%s is a numerical value" % (column))
        if plot == "yes":
            pplot = df[column].hist(figsize=(10, 5), grid=False)
            pplot.set_title("Distribution of %s" % column)
            pplot.set_xlabel(column)
            pplot.set_ylabel("Number of Samples")
    return


def analyze_columns(df, column_list):
    '''
    analyze a list of columns in a dataframe
    '''
    for col in column_list:
        analyze_1_column(df, col)
    return


def get_unique_values(df, column_name):
    '''
    get value_count for a column
    '''
    unique = pd.DataFrame(df[column_name].value_counts().reset_index())
    unique.columns = [column_name, "count"]
    return unique


def get_unique_pair(df, column_list):
    '''
    get status for unique combination of multiple columns
    '''
    unique = df.groupby(column_list).size().reset_index().rename(
        columns={0: "size"})
    return unique


def get_all_stats(df, column_list, column):
    temp = df.groupby(column_list)[[column]].agg(['size', 'mean',
                                                  'median', 'var',
                                                  'std']).reset_index()
    temp = temp.round(3)
    temp.columns = column_list + ['%s_size' % (column), '%s_mean' % (column),
                                  '%s_median' % (column), '%s_var' % (column),
                                  '%s_std' % (column)]
    return temp


def filter_dataframe(df, column_list, value_list, column_to_keep=[]):
    '''
    filter a dataframe with multiple conditions
    '''
    num = len(column_list)
    for i in range(num):
        df = df[df[column_list[i]] == value_list[i]]
    if column_to_keep:
        df = df[column_to_keep]
    return df


def get_filtered_data_list(df, column, value_list, column_to_keep=[]):
    '''
    get a list of dataframe/array with satisfied condition
    '''
    res = []
    for v in value_list:
        if type(v) == list:
            temp = df[df[column].isin(v)]
        else:
            temp = df[df[column] == v]
        if column_to_keep:
            temp = temp[column_to_keep].dropna().values
        res.append(temp)
    return res


def write_readme(src_dir, dst_dir, function):
    if not os.path.exists(dst_dir):
        os.makedirs(dst_dir)
    with open(os.path.join(dst_dir, "readme.txt"), "w") as f:
        f.write("generated by %s from files in %s" % (function, src_dir))


def save_to_individual_file(df, dst_dir, file_prefix):
    '''
    save dataframe to an individual .csv file
    '''
    if not os.path.exists(dst_dir):
        os.makedirs(dst_dir)

    file_name = str.format("%s.csv" % file_prefix)
    df.to_csv(os.path.join(dst_dir, file_name), index=False)


def parallel_process(file_list, function):
    '''
    apply function of all files in file_list
    '''
    pool = Pool(os.cpu_count() - 1)
    pool.map(function, file_list)

    pool.close()
    pool.join()


def compare_id(train, test, id_, dest_dir):
    '''
    get obs with the same id in train and test data and export to two files
    '''
    train_s = train[train["trackingId"] == id_]
    test_s = test[test["trackingId"] == id_]
    train_s = train_s.sort_values(["sourceEventTime", "destinationEventTime"])
    train_s.to_csv(dest_dir + str(id_) + "_train.csv", index=False)
    test_s.to_csv(dest_dir + str(id_) + "_test.csv", index=False)
    return


def get_month_week(df):
    '''
    get month and week for each obs
    '''
    df["Destination Event Month"] = df.destinationEventTime.apply(
        lambda x: str(x)[5:7])
    df["Source Event Month"] = df.sourceEventTime.apply(
        lambda x: str(x)[5:7])
    df["Destination Event Week"] = df.destinationEventTime.apply(
        lambda x: pd.to_datetime(x).weekday())
    df["Source Event Week"] = df.sourceEventTime.apply(
        lambda x: pd.to_datetime(x).weekday())
    return df


def get_mot_perc(df, column1="Source Event Month", column2="modeOfTransport"):
    '''
    calculate percentage of each mode in each Month
    '''
    overall = pd.DataFrame(df.groupby(column1)[[column2]].size())
    overall.columns = ["overall"]

    for m in df[column2].unique():
        temp = pd.DataFrame(df[df[column2] == m].groupby(column1).size())
        temp.columns = [m]
        overall = pd.concat([overall, temp], axis=1)

    for i in overall.columns:
        if i != "overall":
            overall[i] = overall[i] / overall.overall

    overall = overall.drop("overall", axis=1)
    return overall


def get_time_route_id(df, column1="Source Event Month"):
    '''
    first get stats of time duration in each Month
    then get number of routes/OD and trackingId in each Month
    '''
    to_be_merged_df = get_all_stats(df, column1, "duration")

    result = pd.DataFrame()
    result[column1] = to_be_merged_df[column1].unique()
    result['number_of_routes'] = 0
    result['number_of_trackingId'] = 0

    for m in to_be_merged_df[column1].unique():
        r = len(df[df[column1] == m].groupby(["origin", "destination"]).size())
        t = len(df[df[column1] == m].groupby(["trackingId"]).size())
        result.loc[result[column1] == m, "number_of_routes"] = r
        result.loc[result[column1] == m, "number_of_trackingId"] = t

        result = result.merge(to_be_merged_df)
    return result


def get_unique_key_month(df_list, column="key"):
    '''

    '''
    result = pd.DataFrame()
    result["Month"] = range(1, len(df_list) + 1)
    result["number_of_key"] = 0
    result["number_of_unique_key"] = 0
    for i in range(len(df_list)):
        union = set()
        temp = set(df_list[i]["key"].unique())
        result.loc[result["Month"] == i+1, "number_of_key"] = len(temp)
        for j in range(len(df_list)):
            if j != i:
                union = union.union(set(df_list[j]["key"].unique()))
        unique = temp-union
        result.loc[result["Month"] == i+1,
                   "number_of_unique_key"] = len(unique)
    result["perc_unique_key"] = result["number_of_unique_key"] / \
        result["number_of_key"]
    return result


def get_unique_leg_month(df_list):
    '''
    '''
    result = pd.DataFrame()
    result["Month"] = range(1, len(df_list) + 1)
    result["number_of_leg"] = 0
    result["number_of_unique_leg"] = 0
    for i in range(len(df_list)):
        union = set()
        temp = set(df_list[i]["OD"].unique())
        result.loc[result["Month"] == i+1, "number_of_leg"] = len(temp)
        for j in range(len(df_list)):
            if j != i:
                union = union.union(set(df_list[j]["OD"].unique()))
        unique = temp-union
        result.loc[result["Month"] == i+1,
                   "number_of_unique_leg"] = len(unique)
    result["perc_unique_leg"] = result["number_of_unique_leg"] / \
        result["number_of_leg"]
    return result


def get_lat_long(df, loc_df):
    unique_location = np.unique(df[["origin", "destination"]])
    unique_loc_df = loc_df[loc_df.element_id.isin(unique_location)]
    m1 = df.merge(unique_loc_df, left_on=["origin"], right_on="element_id",
                  how="left")
    m1 = m1.drop("element_id", axis=1)
    m2 = m1.merge(unique_loc_df, left_on=["destination"],
                  right_on="element_id", how="left")
    m2 = m2.drop("element_id", axis=1)
    return m2


def get_domestic(df):
    df["domestic_x"] = "no"
    df["domestic_y"] = "no"
    df.loc[(df.latitude_x >= 23.5) & (df.latitude_x <= 50) &
           (df.longitude_x >= -126) & (df.longitude_x <= -65),
           "domestic_x"] = "yes"
    df.loc[(df.latitude_y >= 23.5) & (df.latitude_y <= 50) &
           (df.longitude_y >= -126) & (df.longitude_y <= -65),
           "domestic_y"] = "yes"
    return df


def write_excel(dst_dir):
    writer = pd.ExcelWriter(dst_dir + 'stats_all.xlsx')
    for file in os.listdir(dst_dir):
        file_prefix = os.path.splitext(file)[0]
        if file.endswith("_stats.csv"):
            pd.read_csv(dst_dir + file).to_excel(writer, file_prefix)
    writer.save()
    return
