Conda
conda env create -f environment.yml

————————————————————————————————————————————————————————————————————————————————————

Docker
Install docker
docker build -t computeeta .
docker images
docker run -i -t <imageID>
docker run -i -p 8080:12345 <imageID>

curl http://localhost:<port>/health
If not working: go inside the container
	docker run -i -t <imageID> /bin/bash
docker ps | grep computeeta
docker exec -it <containerID> /bin/bash

Remove image:
	Docker rmi -f <imageID>

————————————————————————————————————————————————————————————————————————————————————
# find file
find . -iname 'spark-defaults.conf'

# get ip addr
ifconfig  -a | grep 10

————————————————————————————————————————————————————————————————————————————————————

Hdfs

Copy data from hfs to ambari:
sudo -u hdfs dfs -copyToLocal /data/datalake/flex/normalized/TRIGGER temp

Make a zip 
zip -r temp.zip temp/ 

Copy data to local
scp wding@int-ambari01.escm.co:~/temp.zip .

Copy data to ambari
scp temp.zip wding@stage-ambari01.escm.co:~/temp

Unzip 
unzip temp.zip

Load data to cluster:
sudo -u hdfs hdfs dfs -put temp /data/datalake/triggers/flextronics/…


scp test.txt wding@int-ambari01.escm.co:/temp

Put data to hfs:
sudo -u hdfs hdfs dfs -put Elem_20180508 /data/datalake/triggers/flextronics


Load data to cluster:
scp test.txt wding@int-ambari01.escm.co:/calceta

sudo -u hdfs hdfs dfs -put Elem_20180508 /data/datalake/triggers/flextronics
sudo -u hdfs hdfs dfs -copyFromLocal test.txt /data/datalake/triggers/flex

Remove
sudo -u hdfs hdfs dfs -rm /data/datalake/triggers/flextronics/*.xlsx
sudo -u hdfs hdfs dfs -rm -r /data/datalake/triggers/flextronics/test

Mkdir
sudo -u hdfs hdfs dfs -mkdir /data/datalake/flex/feed/TRIGGER/INDEPENDENT_DEMAND/SNAPSHOT

sudo -u hdfs hdfs dfs -cp dir1 dir2

Check exsiting file
hdfs dfs -ls /
Absolute path: hdfs dfs -ls hdfs://int-ambari01.escm.co:8020/data/datalake/triggers/flextronics/DEMAND

Copy
hdfs dfs -copyToLocal /data/datalake/flex/normalized/TRIGGER .
————————————————————————————————————————————————————————————————————————————————————

Git
Git clone -b <branch> <url>
git checkout -b <branch>
————————————————————————————————————————————————————————————————————————————————————

Jupyter
Ssh
https://coderwall.com/p/ohk6cg/remote-access-to-ipython-notebooks-via-ssh
remote_user@remote_host$ jupyter notebook --no-browser --port=8889
local_user@local_host$ ssh -N -f -L localhost:4444:localhost:8889 remote_user@remote_host
Specify port: ssh -N -f -L localhost:1234:localhost:8888 -p 55555 emma@24.6.218.37
To close the SSH tunnel on the local machine, look for the process and kill it manually:
jupyter notebook list
fuser 8888/tcp
local_user@local_host$ ps aux | grep localhost:8889
local_user 18418  0.0  0.0  41488   684 ?        Ss   17:27   0:00 ssh -N -f -L localhost:8888:localhost:8889 remote_user@remote_host
local_user 18424  0.0  0.0  11572   932 pts/6    S+   17:27   0:00 grep localhost:8889
local_user@local_host$ kill -15 18418

Fix kernel error
jupyter kernelspec list
rm -r <the above path>
Ls
Count number of files: ls directory | wc -l
Close an open port:
sudo lsof -i :5006
sudo kill -9 PID

Fix package installation error

import sys
sys.executable
sys.version
sys.path

import numpy
numpy.__path__

!source activate py3;pip install scikit-surprise
!{sys.executable} -m pip install scikit-surprise

————————————————————————————————————————————————————————————————————————————————————

Nohup
Nohup python3 <file.py> &

————————————————————————————————————————————————————————————————————————————————————

Spark
Run script on cluster:
sudo -u hdfs /usr/bin/spark-submit --master yarn --deploy-mode cluster sample.py
sudo -u hdfs /usr/bin/spark-submit --master local[*] process-actual-demand_1.py --file_path "HistoricalSalesOrderShipment/fake_old.csv" --output_path "test"
spark-submit --master yarn --deploy-mode cluster --conf spark.yarn.submit.waitAppCompletion=true --num-executors 2 --executor-cores 14 --driver-memory 3g --executor-memory 45g --properties-file /opt/elementum/spark-defaults-int.conf s3://elementum-r9-stage/calceta/spark/src/es_data_extraction_spark/run_all_jenkins_spark.py 20180610 20180611
step function
aws emr add-steps --region us-east-1 --cluster-id j-1I76SLBICAZWW  --steps Type=spark,Name=spark_calceta_ml,Args=[--master,yarn,--deploy-mode,cluster,--conf,spark.yarn.submit.waitAppCompletion=true,--num-executors,2,--executor-cores,14,--driver-memory,3g,--executor-memory,45g,--properties-file,/opt/elementum/spark-defaults-int.conf,s3://elementum-r9-stage/calceta/spark/src/es_data_extraction_spark/run_all_jenkins_spark.py,20180501,20180502,calceta/spark/test_0612_airflow/],ActionOnFailure=CONTINUE
Pyspark
Pip install pyspark
python3 -m pip install ipykernel
python3 -m ipykernel install --user

Running jupyter notebook:
export PYSPARK_DRIVER_PYTHON=ipython3
export PYSPARK_DRIVER_PYTHON_OPTS='notebook'

# Set SPARK_MEM if it isn't already set since we also use it for this process
SPARK_MEM=${SPARK_MEM:-2048m}
export SPARK_MEM

# Set JAVA_OPTS to be able to load native libraries and to set heap size
JAVA_OPTS="$OUR_JAVA_OPTS"
JAVA_OPTS="$JAVA_OPTS -Djava.library.path=$SPARK_LIBRARY_PATH"
JAVA_OPTS="$JAVA_OPTS -Xms$SPARK_MEM -Xmx$SPARK_MEM"

Then type pyspark --master local[*]  in terminal: run notebook
Running Spark submit
export PYSPARK_PYTHON=/usr/bin/python3
export PYSPARK_DRIVER_PYTHON=/usr/bin/python3

————————————————————————————————————————————————————————————————————————————————————

tar -xzf file.tar.gz

————————————————————————————————————————————————————————————————————————————————————

Virtual environment
PATH=$WORKSPACE/venv/bin:/usr/local/bin:$PATH

if [ ! -f verify_calcETA_data/bin/activate ]; then
  echo "Virtal Env. not found, creating..."
  virtualenv --python=/usr/bin/python3 --no-site-packages verify_calcETA_data
fi
source verify_calcETA_data/bin/activate
which pip

pip install --upgrade pip
pip install -U --force numpy
pip install -U --force scipy==0.18.1
pip install elasticsearch
pip install requests
pip install -U --force pandas==0.19.2
pip install boto
pip install boto3
pip install -U --force sklearn
pip install pyspark
